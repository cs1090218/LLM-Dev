{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on:\n",
    "https://docs.llamaindex.ai/en/stable/presentations/materials/2024-02-28-rag-bootcamp-vector-institute/?h=rag\n",
    "\n",
    "- If using Ollama LLM and embeddings, feel free to use this notebook as is by setting USE_OPENAI = False\n",
    "- If using OpenAI LLM and embeddings, use the llamaindex_using_pickledata.ipynb since that will save requesting embeddings again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_OPENAI = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "if USE_OPENAI:\n",
    "    Settings.llm = OpenAI(model=\"gpt-3.5-turbo\", api_key=os.getenv('OPENAI_API_KEY'))\n",
    "    Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")\n",
    "else:\n",
    "    Settings.llm = Ollama(model=\"llama3:instruct\", request_timeout=120.0)\n",
    "    Settings.embed_model = OllamaEmbedding(\n",
    "        model_name=\"llama3:instruct\",\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        ollama_additional_kwargs={\"mirostat\": 0})\n",
    "\n",
    "Settings.node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load the data.\n",
    "\n",
    "With llama-index, before any transformations are applied,\n",
    "data is loaded in the `Document` abstraction, which is\n",
    "a container that holds the text of the document.\n",
    "\"\"\"\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "loader = SimpleDirectoryReader(input_files=[\"./data/idpp.pdf\", \"./data/metagpt.pdf\", \"./data/state_of_the_union.txt\"]) # input_dir=\"./data\")\n",
    "documents = loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to see what the text looks like\n",
    "# print (documents[0].text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Chunk, Encode, and Store into a Vector Store.\n",
    "\n",
    "# To streamline the process, we can make use of the IngestionPipeline\n",
    "# class that will apply your specified transformations to the\n",
    "# Document's.\n",
    "# \"\"\"\n",
    "\n",
    "# from llama_index.core.ingestion import IngestionPipeline\n",
    "# from llama_index.core.node_parser import SentenceSplitter\n",
    "# from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "# import qdrant_client\n",
    "\n",
    "# client = qdrant_client.QdrantClient(location=\":memory:\")\n",
    "# vector_store = QdrantVectorStore(client=client, collection_name=\"test_store\")\n",
    "\n",
    "# pipeline = IngestionPipeline(\n",
    "#     transformations=[\n",
    "#         SentenceSplitter(),\n",
    "#         Settings.embed_model,\n",
    "#     ],\n",
    "#     vector_store=vector_store,\n",
    "# )\n",
    "# _nodes = pipeline.run(documents=documents, num_workers=4)\n",
    "\n",
    "# \"\"\"Create a llama-index... wait for it... Index.\n",
    "\n",
    "# After uploading your encoded documents into your vector\n",
    "# store of choice, you can connect to it with a VectorStoreIndex\n",
    "# which then gives you access to all of the llama-index functionality.\n",
    "# \"\"\"\n",
    "\n",
    "# from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n",
    "# print (len(_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the index - Double Check the path\n",
    "# index.storage_context.persist(\"models/openAI_idpp_metagpt_state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Retrieve relevant documents against a query.\n",
    "\n",
    "With our Index ready, we can now query it to\n",
    "retrieve the most relevant document chunks.\n",
    "\"\"\"\n",
    "\n",
    "retriever = index.as_retriever(similarity_top_k=2)\n",
    "retrieved_nodes = retriever.retrieve(\"What did the president say about Justice Breyer?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And we will save democracy.\n",
      "\n",
      "As hard as those times have been, I’m more optimistic about America today than I’ve been my whole life because I see the future that’s within our grasp, because I know there is simply nothing beyond our camas- — our capacity.\n",
      "\n",
      "We’re the only nation on Earth that has always turned every crisis we’ve faced into an opportunity, the only nation that can be defined by a single word: possibilities.\n",
      "\n",
      "So, on this night, on our 245th year as a nation, I’ve come to report on the state of the nation — the state of the union. And my report is this: The State of the Union is strong because you, the American people, are strong.\n",
      "\n",
      "We are stronger today — we are stronger today than we were a year ago. And we’ll be stronger a year from now than we are today.\n",
      "\n",
      "This is our moment to meet and overcome the challenges of our time. And we will, as one people, one America — the United States of America.\n",
      "\n",
      "God bless you all. And may God protect our troops. Thank you. Go get ’em.\n",
      "================\n",
      "He rejected repeated efforts at diplomacy.\n",
      "\n",
      "He thought the West and NATO wouldn’t respond. He thought he could divide us at home, in this chamber, in this nation. He thought he could divide us in Europe as well.\n",
      "\n",
      "But Putin was wrong. We are ready. We are united. And that’s what we did: We stayed united.\n",
      "\n",
      "We prepared extensively and carefully. We spent months building coalitions of other freedom-loving nations in Europe and the Americas to — from America to the Asian and African continents to confront Putin.\n",
      "\n",
      "Like many of you, I spent countless hours unifying our European Allies.\n",
      "\n",
      "We shared with the world, in advance, what we knew Putin was planning and precisely how he would try to falsely and justify his aggression.\n",
      "\n",
      "We countered Russia’s lies with the truth. And now — now that he’s acted, the free world is holding him accountable, along with 27 members of the European Union — including France, Germany, Italy — as well as countries like the United Kingdom, Canada, Japan, Korea, Australia, New Zealand, and many others. Even Switzerland are inflicting pain on Russia and supporting the people of Ukraine.\n",
      "\n",
      "Putin is now isolated from the world more than he has ever been.\n",
      "\n",
      "Together. Together. Together, along with our Allies, we are right now enforcing powerful economic sanctions. We’re cutting off Russia’s largest banks from the international financial system; preventing Russia’s Central Bank from defending the Russian ruble, making Putin’s $630 billion war fund worthless. We’re choking Russia’s access, we’re choking Russia’s access to technology that will sap its economic strength and weaken its military for years to come.\n",
      "\n",
      "Tonight, I say to the Russian oligarchs and the corrupt leaders who’ve bilked billions of dollars off this violent regime: No more.\n",
      "\n",
      "The United States — I mean it. The United States Department of Justice is assembling a dedicated task force to go after the crimes of the Russian oligarchs.\n",
      "\n",
      "We’re joining with European Allies to find and seize their yachts, their luxury apartments, their private jets. We’re coming for your ill-begotten gains.\n",
      "\n",
      "And, tonight, I’m announcing that we will join our Allies in closing off American air space to all Russian flights, further isolating Russia and adding an additional squeeze on their economy.\n",
      "\n",
      "He has no idea what’s coming.\n"
     ]
    }
   ],
   "source": [
    "# to view the retrieved node\n",
    "print (retrieved_nodes[0].text)\n",
    "print (\"================\")\n",
    "print (retrieved_nodes[1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Context-Augemented Generation.\n",
    "\n",
    "With our Index ready, we can create a QueryEngine\n",
    "that handles the retrieval and context augmentation\n",
    "in order to get the final response.\n",
    "\"\"\"\n",
    "\n",
    "query_engine = index.as_query_engine(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "{context_str}\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: {query_str}\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "# to inspect the default prompt being used\n",
    "print(\n",
    "    query_engine.get_prompts()[\n",
    "        \"response_synthesizer:text_qa_template\"\n",
    "    ].default_template.template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided context, the models tried for predicting ALS progression in the idpp paper include:\n",
      "\n",
      "* T1 (Task1) with median sensor features or TsFresh features\n",
      "* T1+2 Lasso (using both Task1 and Task2 data for training)\n",
      "* EN (ElasticNet) model\n",
      "* Lasso model\n",
      "* Naive model\n",
      "* ElasticNet + Naive model (the final model that achieved the best performance)\n",
      "\n",
      "These models were used separately to predict ALSFRS-R scores for each of the 12 questions.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What were the models tried for predicting ALS progression in the idpp paper?.\")\n",
    "print (response)\n",
    "# According to the provided context, several machine learning algorithms for regression as well as a Long Short-Term Memory (LSTM) neural network were explored to model the temporal dependencies in the sequential sensor data. The naive model was also tried, which simply carries the last observed value forward. Additionally, ElasticNet and Lasso models were used with different subsets of Task1 and Task2 data for training.\n",
    "# The models tried for predicting ALS progression in the iDPP paper included a naive model that carried the last observed value forward, various Machine Learning algorithms for regression, and a Long Short-Term Memory (LSTM) neural network to model the temporal dependencies in the sequential sensor data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation strategy used by the authors in the idpp paper was not explicitly mentioned. However, based on the provided table, it can be inferred that the authors used a five-fold cross-validation approach for evaluating their models' performance.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What was the validation strategy used by the authors in the idpp paper?.\")\n",
    "print (response)\n",
    "# The validation strategy used by the authors in the iDPP paper is not explicitly mentioned. However, based on the provided context, it can be inferred that the authors evaluated the performance of their models using Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) metrics, as identified by the challenge organizers. The authors also report the results of their experiments, comparing the performance of different models, which suggests a hold-out set or cross-validation approach was used for model evaluation.\n",
    "# The authors in the idpp paper used a nested k-fold cross-validation strategy for their validation. This strategy consisted of two loops - an inner loop and an outer loop. In the inner loop, a test set containing 10% of complete patient data was set aside, while the remaining data underwent further k-fold cross-validation. This was adapted to ensure that each patient's complete set of observations was included in both the training and validation sets. The outer loop repeated the same procedure on another test set covering another 10% of the patients. This process was repeated for 10 iterations in the outer loop to compute RMSE for model selection. The best hyperparameters were chosen based on these iterations, and all the data was fit using those hyperparameters prior to the final submission.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the given context, the ElasticNet + Naive model achieved an RMSE of 0.5048. However, it's mentioned that this model was \"just shy\" of the Naive model only submission which had a lower RMSE of 0.4912. Therefore, the Naive model only submission performed the best with the lowest RMSE in the idpp paper.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Which model performed the best with lowest RMSE in the idpp paper?.\")\n",
    "print (response)\n",
    "# According to Table 2 in the provided context, the best-performing model for each of the 12 questions in ALSFRS-R scores is denoted by a green box. However, it's not specified which model performed the best overall with the lowest RMSE.\n",
    "# The ElasticNet + Naive model performed the best with the lowest RMSE in the iDPP paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm happy to help! However, I must point out that there is no mention of President or Justice Breyer in the provided context. The conversation appears to be about a research project involving data analysis and visualization, with no reference to politics or any specific individual. Therefore, I cannot provide an answer to this query as it is not related to the given context.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What did the president say about Justice Breyer\")\n",
    "print (response)\n",
    "# I apologize, but there is no mention of a president or Justice Breyer in the provided context. The text appears to be discussing a study on predicting ALSFRS-R scores based on time-series sensor data and does not contain any information related to politics or justices. Therefore, I cannot provide an answer to this query as it is outside the scope of the given context.\n",
    "# The president expressed gratitude and appreciation for Justice Breyer's service to the country, acknowledging his dedication as an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In MetaGPT, agents play different roles in a straightforward role-play framework for programming. This communication between agents enables them to share information and work together effectively.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"How do agents share information with other agents in MetaGPT?\")\n",
    "print (response)\n",
    "# According to the provided context, MetaGPT is a meta-programming framework for multi-agent collaboration based on Large Language Models (LLMs). It has well-defined functions like role definition and message sharing, making it a useful platform for developing LLM-based multi-agent systems. However, it does not explicitly mention how agents share information with other agents.\n",
    "# Agents share information with other agents by utilizing a shared message pool. This shared message pool allows all agents to exchange messages directly. Agents publish their structured messages in the pool and can also access messages from other entities transparently. This system enables any agent to retrieve necessary information directly from the shared pool without having to inquire about other agents and wait for their responses, ultimately enhancing communication efficiency.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
