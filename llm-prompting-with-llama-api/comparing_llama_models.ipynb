{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fde90571-0799-48af-b315-247f85c2269b",
   "metadata": {},
   "source": [
    "# Comparing Llama Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce53734-47df-454d-bb29-c135b2a1b338",
   "metadata": {},
   "source": [
    "- Load helper function to prompt Llama models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "469f76b7-acd9-4192-93f2-a37d4ac9d196",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import llama helper function\n",
    "from utils import llama, llama_chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cd039f-d768-44a7-b81b-bfab14a33a90",
   "metadata": {},
   "source": [
    "### Task 1: Sentiment Classification\n",
    "- Compare the models on few-shot prompt sentiment classification.\n",
    "- You are asking the model to return a one word response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44d44286-dd86-48bb-9457-72dca64b5b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''\n",
    "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
    "Sentiment: Positive\n",
    "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
    "Sentiment: Negative\n",
    "Message: Can't wait to order pizza for dinner tonight!\n",
    "Sentiment: ?\n",
    "\n",
    "Give a one word response.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38108527-032d-411b-bb3e-31471694758f",
   "metadata": {},
   "source": [
    "- First, use the 8B parameter chat model (`meta-llama/Llama-3-8b-chat-hf`) to get the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d58a65f2-a8e2-4cf4-abc4-c65fd64710c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "response = llama(prompt,\n",
    "                 model = \"meta-llama/Llama-3-8b-chat-hf\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24729f9d-0899-472b-9025-12824b962a52",
   "metadata": {},
   "source": [
    "- Now, use the 70B parameter chat model (`meta-llama/Llama-3-70b-chat-hf`) on the same task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f62ef43-0b6a-4c11-bed9-8f49ec4f0802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "response = llama(prompt,\n",
    "                 model=\"meta-llama/Llama-3-70b-chat-hf\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0c5ded-3bbe-4cac-898c-578655ea47cc",
   "metadata": {},
   "source": [
    "### Task 2: Summarization\n",
    "- Compare the models on summarization task.\n",
    "- This is the same \"email\" as the one you used previously in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07d248f0-a610-4696-aacd-12ca94b2fb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "email = \"\"\"\n",
    "Dear Amit,\n",
    "\n",
    "An increasing variety of large language models (LLMs) are open source, or close to it. The proliferation of models with relatively permissive licenses gives developers more options for building applications.\n",
    "\n",
    "Here are some different ways to build applications based on LLMs, in increasing order of cost/complexity:\n",
    "\n",
    "Prompting. Giving a pretrained LLM instructions lets you build a prototype in minutes or hours without a training set. Earlier this year, I saw a lot of people start experimenting with prompting, and that momentum continues unabated. Several of our short courses teach best practices for this approach.\n",
    "One-shot or few-shot prompting. In addition to a prompt, giving the LLM a handful of examples of how to carry out a task â€” the input and the desired output â€” sometimes yields better results.\n",
    "Fine-tuning. An LLM that has been pretrained on a lot of text can be fine-tuned to your task by training it further on a small dataset of your own. The tools for fine-tuning are maturing, making it accessible to more developers.\n",
    "Pretraining. Pretraining your own LLM from scratch takes a lot of resources, so very few teams do it. In addition to general-purpose models pretrained on diverse topics, this approach has led to specialized models like BloombergGPT, which knows about finance, and Med-PaLM 2, which is focused on medicine.\n",
    "For most teams, I recommend starting with prompting, since that allows you to get an application working quickly. If youâ€™re unsatisfied with the quality of the output, ease into the more complex techniques gradually. Start one-shot or few-shot prompting with a handful of examples. If that doesnâ€™t work well enough, perhaps use RAG (retrieval augmented generation) to further improve prompts with key information the LLM needs to generate high-quality outputs. If that still doesnâ€™t deliver the performance you want, then try fine-tuning â€” but this represents a significantly greater level of complexity and may require hundreds or thousands more examples. To gain an in-depth understanding of these options, I highly recommend the course Generative AI with Large Language Models, created by AWS and DeepLearning.AI.\n",
    "\n",
    "(Fun fact: A member of the DeepLearning.AI team has been trying to fine-tune Llama-2-7B to sound like me. I wonder if my job is at risk? ðŸ˜œ)\n",
    "\n",
    "Additional complexity arises if you want to move to fine-tuning after prompting a proprietary model, such as GPT-4, thatâ€™s not available for fine-tuning. Is fine-tuning a much smaller model likely to yield superior results than prompting a larger, more capable model? The answer often depends on your application. If your goal is to change the style of an LLMâ€™s output, then fine-tuning a smaller model can work well. However, if your application has been prompting GPT-4 to perform complex reasoning â€” in which GPT-4 surpasses current open models â€” it can be difficult to fine-tune a smaller model to deliver superior results.\n",
    "\n",
    "Beyond choosing a development approach, itâ€™s also necessary to choose a specific model. Smaller models require less processing power and work well for many applications, but larger models tend to have more knowledge about the world and better reasoning ability. Iâ€™ll talk about how to make this choice in a future letter.\n",
    "\n",
    "Keep learning!\n",
    "\n",
    "Andrew\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize this email and extract some key points.\n",
    "\n",
    "What did the author say about llama models?\n",
    "```\n",
    "{email}\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135f00bb-51d9-492a-920b-5b286ab5f5c4",
   "metadata": {},
   "source": [
    "- First, use the 8B parameter chat model (`meta-llama/Llama-3-8b-chat-hf`) to summarize the email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99c60da6-9b3e-4f11-a1d5-8e5e273e91ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "\n",
      "The email discusses the increasing availability of large language models (LLMs) with permissive licenses, which provides developers with more options for building applications. The author, Andrew, outlines different ways to build applications based on LLMs, ranging from prompting to fine-tuning, and provides guidance on when to use each approach. He also mentions the importance of choosing the right model and notes that a future letter will cover this topic.\n",
      "\n",
      "Key points:\n",
      "\n",
      "* LLMs with permissive licenses provide developers with more options for building applications.\n",
      "* Prompting is a simple and quick way to build an application, but may not produce high-quality results.\n",
      "* One-shot or few-shot prompting can improve results by providing the LLM with examples of how to carry out a task.\n",
      "* Fine-tuning an LLM requires more resources and expertise, but can produce high-quality results.\n",
      "* Pretraining an LLM from scratch is resource-intensive and typically only done by a few teams.\n",
      "* Choosing the right model is important, with smaller models requiring less processing power and larger models having more knowledge and better reasoning ability.\n",
      "* The author recommends starting with prompting and gradually moving to more complex techniques if needed.\n",
      "* Fine-tuning a proprietary model, such as GPT-4, can be challenging and may not produce superior results compared to prompting a larger model.\n"
     ]
    }
   ],
   "source": [
    "response_llama3_8b = llama(prompt,\n",
    "                model=\"meta-llama/Llama-3-8b-chat-hf\")\n",
    "print(response_llama3_8b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aa0c61-ff78-4372-b4fb-a51be7bf11eb",
   "metadata": {},
   "source": [
    "- Lastly, use the 70B parameter chat model (`meta-llama/Llama-3-70b-chat-hf`) to summarize the email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a031c7e-d4d0-404d-867c-27f8a0ab96eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a summary of the email and some key points:\n",
      "\n",
      "**Summary:** The email discusses the increasing availability of open-source large language models (LLMs) and the various ways to build applications using them, ranging from simple prompting to fine-tuning and pretraining. The author recommends starting with prompting and gradually moving to more complex techniques if needed.\n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "1. Open-source LLMs are becoming more available, giving developers more options for building applications.\n",
      "2. There are four ways to build applications using LLMs, in increasing order of cost/complexity: prompting, one-shot or few-shot prompting, fine-tuning, and pretraining.\n",
      "3. Prompting is a quick and easy way to build a prototype, while fine-tuning and pretraining require more resources and expertise.\n",
      "4. The choice of development approach depends on the application and the desired output.\n",
      "5. Smaller models require less processing power and may be suitable for many applications, while larger models have more knowledge and better reasoning ability.\n",
      "6. The author mentions a course, \"Generative AI with Large Language Models\", as a resource for learning more about these options.\n",
      "7. There is a mention of \"Llama-2-7B\" model, which is being fine-tuned to sound like the author, raising a humorous question about job security.\n"
     ]
    }
   ],
   "source": [
    "response_llama3_70b = llama(prompt,\n",
    "                model=\"meta-llama/Llama-3-70b-chat-hf\")\n",
    "print(response_llama3_70b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6f051d-a7e4-45cc-b162-646862084994",
   "metadata": {},
   "source": [
    "#### Model-Graded Evaluation: Summarization\n",
    "\n",
    "- Interestingly, you can ask a LLM to evaluate the responses of other LLMs.\n",
    "- This is known as **Model-Graded Evaluation**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a77de7b-7339-4ff3-b8fd-63cb448d4ed8",
   "metadata": {},
   "source": [
    "- Create a `prompt` that will evaluate these three responses using 70B parameter chat model (`meta-llama/Llama-3-70b-chat-hf`).\n",
    "- In the `prompt`, provide the \"email\", \"name of the models\", and the \"summary\" generated by each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e10e1d72-10ab-43dd-80f8-74fcf57e28a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Evaluation of each model's summary:**\n",
      "\n",
      "**Model: llama-3-8b-chat**\n",
      "\n",
      "* The summary provides a good overview of the original text, covering the main points and key ideas.\n",
      "* The summary follows the instructions of the prompt, providing a concise summary of the email.\n",
      "* The key points section is well-organized and easy to follow, highlighting the main takeaways from the email.\n",
      "* One interesting characteristic of this model's output is the use of a clear and concise structure, with a separate section for the summary and key points.\n",
      "\n",
      "**Model: llama-3-70b-chat**\n",
      "\n",
      "* The summary is also a good representation of the original text, covering the main ideas and key points.\n",
      "* The summary follows the instructions of the prompt, providing a concise summary of the email.\n",
      "* The key points section is well-organized and easy to follow, highlighting the main takeaways from the email.\n",
      "* One interesting characteristic of this model's output is the use of numbered key points, which makes it easy to scan and understand.\n",
      "\n",
      "**Comparison of models and recommendation:**\n",
      "\n",
      "Both models perform well in summarizing the original text and following the instructions of the prompt. However, there are some differences in the structure and organization of their outputs.\n",
      "\n",
      "The llama-3-8b-chat model uses a clear and concise structure, with a separate section for the summary and key points. This makes it easy to read and understand. The key points section is also well-organized and easy to follow.\n",
      "\n",
      "The llama-3-70b-chat model uses numbered key points, which makes it easy to scan and understand. However, the structure of the output is not as clear and concise as the llama-3-8b-chat model.\n",
      "\n",
      "Based on these evaluations, I would recommend the llama-3-8b-chat model as the top performer. Its output is well-organized, easy to read, and provides a clear summary of the original text. However, both models are capable of producing high-quality summaries, and the choice between them ultimately depends on personal preference.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Given the original text denoted by `email`\n",
    "and the name of several models: `model:<name of model>\n",
    "as well as the summary generated by that model: `summary`\n",
    "\n",
    "Provide an evaluation of each model's summary:\n",
    "- Does it summarize the original text well?\n",
    "- Does it follow the instructions of the prompt?\n",
    "- Are there any other interesting characteristics of the model's output?\n",
    "\n",
    "Then compare the models based on their evaluation \\\n",
    "and recommend the models that perform the best.\n",
    "\n",
    "email: ```{email}```\n",
    "\n",
    "model: llama-3-8b-chat\n",
    "summary: {response_llama3_8b}\n",
    "\n",
    "model: llama-3-70b-chat\n",
    "summary: {response_llama3_70b}\n",
    "\"\"\"\n",
    "\n",
    "response_eval = llama(prompt,\n",
    "                model=\"meta-llama/Llama-3-70b-chat-hf\")\n",
    "print(response_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179a5581-bb28-4eba-be19-045be0779ed0",
   "metadata": {},
   "source": [
    "### Task 3: Reasoning ###\n",
    "- Compare the three models' performance on reasoning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f98a66a-6d17-4a6d-bae8-881a5d5a1d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "Jeff and Tommy are neighbors\n",
    "\n",
    "Tommy and Eddy are not neighbors\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff5198c1-701d-4e07-93a1-38ab7a12e07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "Are Jeff and Eddy neighbors?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da3a23b9-0dde-4cc2-83cc-b9344b5983d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Given this context: ```{context}```,\n",
    "\n",
    "and the following query:\n",
    "```{query}```\n",
    "\n",
    "Please answer the questions in the query and explain your reasoning.\n",
    "If there is not enough informaton to answer, please say\n",
    "\"I do not have enough information to answer this questions.\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f958db46-abe8-4518-8db4-8829f00cde61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I do not have enough information to answer this question.\n",
      "\n",
      "The query asks if Jeff and Eddy are neighbors, but there is no information provided about their relationship. We know that Jeff and Tommy are neighbors, and Tommy and Eddy are not neighbors, but we don't know anything about Jeff and Eddy's relationship. Therefore, I cannot determine whether they are neighbors or not.\n"
     ]
    }
   ],
   "source": [
    "response_llama3_8b_chat = llama(prompt,\n",
    "                        model=\"META-LLAMA/Llama-3-8B-CHAT-HF\")\n",
    "print(response_llama3_8b_chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d7ea0b1-c040-4fb3-96c7-676e6f64442a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the given context, I can conclude that:\n",
      "\n",
      "Jeff and Eddy are not neighbors.\n",
      "\n",
      "Here's my reasoning:\n",
      "\n",
      "1. We know that Jeff and Tommy are neighbors.\n",
      "2. We also know that Tommy and Eddy are not neighbors.\n",
      "3. Since Tommy is a neighbor of Jeff, but not a neighbor of Eddy, it implies that Jeff and Eddy are not neighbors.\n",
      "\n",
      "So, I can confidently answer that Jeff and Eddy are not neighbors.\n"
     ]
    }
   ],
   "source": [
    "response_llama3_70b_chat = llama(prompt,\n",
    "                        model=\"META-LLAMA/Llama-3-70B-CHAT-HF\")\n",
    "print(response_llama3_70b_chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1017fdb2-3e5d-4f4b-862d-acdb3dc41e5c",
   "metadata": {},
   "source": [
    "#### Model-Graded Evaluation: Reasoning\n",
    "\n",
    "- Again, ask a LLM to compare the three responses.\n",
    "- Create a `prompt` that will evaluate these three responses using 70B parameter chat model (`meta-llama/Llama-3-70b-chat-hf`).\n",
    "- In the `prompt`, provide the `context`, `query`,\"name of the models\", and the \"response\" generated by each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c742104-ef6d-43f6-afa0-6f8ae67ace4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Given the context `context:`,\n",
    "Also also given the query (the task): `query:`\n",
    "and given the name of several models: `mode:<name of model>,\n",
    "as well as the response generated by that model: `response:`\n",
    "\n",
    "Provide an evaluation of each model's response:\n",
    "- Does it answer the query accurately?\n",
    "- Does it provide a contradictory response?\n",
    "- Are there any other interesting characteristics of the model's output?\n",
    "\n",
    "Then compare the models based on their evaluation \\\n",
    "and recommend the models that perform the best.\n",
    "\n",
    "context: ```{context}```\n",
    "\n",
    "model: llama-3-8b-chat\n",
    "response: ```{response_llama3_8b_chat}```\n",
    "\n",
    "model: llama-3-70b-chat\n",
    "response: ``{response_llama3_70b_chat}``\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc6f1cde-68e2-4e67-96da-a98dd2e6bc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the evaluation of each model's response:\n",
      "\n",
      "**Model: llama-3-8b-chat**\n",
      "\n",
      "* Does it answer the query accurately? No, it doesn't provide a direct answer to the query.\n",
      "* Does it provide a contradictory response? No, it doesn't provide a contradictory response, but it doesn't provide a conclusive answer either.\n",
      "* Are there any other interesting characteristics of the model's output? The model correctly identifies the lack of information about Jeff and Eddy's relationship and refuses to make an assumption.\n",
      "\n",
      "**Model: llama-3-70b-chat**\n",
      "\n",
      "* Does it answer the query accurately? Yes, it provides a correct answer to the query based on the given context.\n",
      "* Does it provide a contradictory response? No, it doesn't provide a contradictory response.\n",
      "* Are there any other interesting characteristics of the model's output? The model provides a clear and logical reasoning to support its answer, which is a desirable trait in a language model.\n",
      "\n",
      "Comparison and Recommendation:\n",
      "\n",
      "Based on the evaluation, the **llama-3-70b-chat** model performs better than the **llama-3-8b-chat** model. The llama-3-70b-chat model provides a correct and well-reasoned answer to the query, whereas the llama-3-8b-chat model fails to provide a direct answer.\n",
      "\n",
      "I would recommend using the **llama-3-70b-chat** model for this type of task, as it demonstrates a better ability to reason and draw conclusions from the given context.\n"
     ]
    }
   ],
   "source": [
    "response_eval = llama(prompt, \n",
    "                      model=\"META-LLAMA/Llama-3-70B-CHAT-HF\")\n",
    "\n",
    "print(response_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1c20b2-599d-49ac-bca0-573360051a05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
