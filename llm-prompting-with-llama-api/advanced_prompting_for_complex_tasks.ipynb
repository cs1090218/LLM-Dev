{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57fee1e7-081e-4c22-af70-9839fcfcec32",
   "metadata": {},
   "source": [
    "# Lesson 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3336b23c-be24-4299-82d1-4ba0c578385d",
   "metadata": {},
   "source": [
    "### Import helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2df73149-4fad-4f63-aa85-d338cbffe23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import llama helper function\n",
    "from utils import llama, llama_chat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7faef740-19bd-421e-a985-43217d6d6825",
   "metadata": {},
   "source": [
    "### In-Context Learning\n",
    "\n",
    "#### Standard prompt with instruction\n",
    "- So far, you have been stating the instruction explicitly in the prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ea534c7-e213-417f-a99c-0f8dba92387e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment of this message is POSITIVE. The use of the word \"thoughtful\" implies that the birthday card was well-thought-out and considerate, and the phrase \"thanks\" expresses gratitude and appreciation. Overall, the tone is friendly and celebratory.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "What is the sentiment of:\n",
    "Hi Amit, thanks for the thoughtful birthday card!\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45096c64-f483-4d01-b64c-8de1fb529441",
   "metadata": {},
   "source": [
    "### Zero-shot Prompting\n",
    "- Here is an example of zero-shot prompting.\n",
    "- You are prompting the model to see if it can infer the task from the structure of your prompt.\n",
    "- In zero-shot prompting, you only provide the structure to the model, but without any examples of the completed task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "549667d6-6b23-46a9-8c4a-f089f7936392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment of this message is POSITIVE. The message is expressing gratitude and appreciation for the birthday card, indicating a warm and friendly tone.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
    "Sentiment: ?\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b162fb3d-7020-47c6-bb08-5ed6f138da66",
   "metadata": {},
   "source": [
    "### Few-shot Prompting\n",
    "- Here is an example of few-shot prompting.\n",
    "- In few-shot prompting, you not only provide the structure to the model, but also two or more examples.\n",
    "- You are prompting the model to see if it can infer the task from the structure, as well as the examples in your prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd349402-904a-4e33-9e7c-88a41afc5ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
    "Sentiment: Negative\n",
    "\n",
    "Message: Can't wait to order pizza for dinner tonight\n",
    "Sentiment: Positive\n",
    "\n",
    "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
    "Sentiment: ?\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41560853-9639-4329-87ac-640620bb4b39",
   "metadata": {},
   "source": [
    "### Specifying the Output Format\n",
    "- You can also specify the format in which you want the model to respond.\n",
    "- In the example below, you are asking to \"give a one word response\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f2669f8-9673-4568-aec2-4b9f13d033ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grateful\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
    "Sentiment: Negative\n",
    "\n",
    "Message: Can't wait to order pizza for dinner tonight\n",
    "Sentiment: Positive\n",
    "\n",
    "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
    "Sentiment: ?\n",
    "\n",
    "Give a one word response.\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6e07176-5730-4cc0-bec1-cb3ae9000806",
   "metadata": {},
   "source": [
    "**Note:** For all the examples above, you used the 7 billion parameter model, `llama-2-7b-chat`. And as you saw in the last example, the 7B model was uncertain about the sentiment.\n",
    "\n",
    "- You can use the larger (70 billion parameter) `llama-2-70b-chat` model to see if you get a better, certain response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "415739fe-dcfe-4183-a5d8-fa3ce19aa480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
    "Sentiment: Negative\n",
    "\n",
    "Message: Can't wait to order pizza for dinner tonight\n",
    "Sentiment: Positive\n",
    "\n",
    "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
    "Sentiment: ?\n",
    "\n",
    "Give a one word response.\n",
    "\"\"\"\n",
    "response = llama(prompt,\n",
    "                model=\"meta-llama/Llama-3-70b-chat-hf\")\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe196c59-2c42-4bdb-b7e7-a2f431ffce29",
   "metadata": {},
   "source": [
    "- Now, use the smaller model again, but adjust your prompt in order to help the model to understand what is being expected from it.\n",
    "- Restrict the model's output format to choose from `positive`, `negative` or `neutral`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74528daa-90be-4208-98ac-3cf71dc687b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
    "Sentiment: Negative\n",
    "\n",
    "Message: Can't wait to order pizza for dinner tonight\n",
    "Sentiment: Positive\n",
    "\n",
    "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
    "Sentiment: \n",
    "\n",
    "Respond with either positive, negative, or neutral.\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47d3d067-2b77-4a35-b95b-d7ed9c5bc77f",
   "metadata": {},
   "source": [
    "### Role Prompting\n",
    "- Roles give context to LLMs what type of answers are desired.\n",
    "- Llama often gives more consistent responses when provided with a role.\n",
    "- First, try standard prompt and see the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a94c6ba-5dd7-4a60-9dfe-c878c44fa44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What a profound and age-old question! There is no one definitive answer, as the meaning of life is a deeply personal and subjective concept that can vary greatly from person to person. However, here are some possible ways to approach this question:\n",
      "\n",
      "1. **Reflect on your values and passions**: What gives your life meaning and purpose? What are your core values, and how do they guide your decisions and actions? What are you passionate about, and how do you pursue those passions?\n",
      "2. **Explore philosophical perspectives**: There are many philosophical theories about the meaning of life. For example, some argue that life has no inherent meaning and that we must create our own meaning through our choices and actions (e.g., existentialism). Others believe that life has a inherent meaning, such as the pursuit of happiness, fulfillment, or spiritual growth (e.g., hedonism, stoicism).\n",
      "3. **Consider the human experience**: What is it about being human that gives life meaning? Is it our capacity for love, creativity, or connection with others? Is it our ability to learn, grow, and evolve as individuals?\n",
      "4. **Look to spirituality or religion**: Many people find meaning and purpose in their lives through their spiritual or religious beliefs. What do you believe about the nature of the universe, the purpose of human existence, and the afterlife?\n",
      "5. **Seek out the wisdom of others**: Read books, articles, or quotes from philosophers, spiritual leaders, or everyday people who have grappled with this question. You might find inspiration or new perspectives to consider.\n",
      "6. **Explore the concept of \"eudaimonia\"**: In ancient Greek philosophy, eudaimonia referred to a state of flourishing, happiness, or well-being that arises from living a virtuous and fulfilling life. This concept can be seen as a way to approach the question of the meaning of life.\n",
      "7. **Be honest and authentic**: Ultimately, the meaning of life is a deeply personal and subjective question. You might not have a definitive answer, and that's okay. You can simply be honest about your own struggles and uncertainties, and acknowledge that the search for meaning is an ongoing and evolving process.\n",
      "\n",
      "Here's an example response you could give to your friend:\n",
      "\n",
      "\"I think the meaning of life is a complex and multifaceted question that can't be reduced to a single answer. For me, it's about living a life that aligns with my values and passions, and finding ways to make a positive impact on the world. It's about cultivating meaningful relationships, pursuing my goals and aspirations, and finding joy and fulfillment in the present moment. Ultimately, I think the meaning of life is something that each of us must discover for ourselves, and it's a journey that evolves over time as we grow and learn.\"\n",
      "\n",
      "Remember, there is no one \"right\" answer to this question, and it's okay to not have all the answers. The important thing is to approach the question with an open mind, a willingness to learn, and a commitment to living a life that is authentic and meaningful to you.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "How can I answer this question from my friend:\n",
    "What is the meaning of life?\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42594189-5692-4ab3-9a1b-3d6ac58c8a78",
   "metadata": {},
   "source": [
    "- Now, try it by giving the model a \"role\", and within the role, a \"tone\" using which it should respond with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2db7b960-d6f6-48a0-8c62-a68c999ad4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrr, shiver me timbers! Ye be askin' the big question, matey! The meaning o' life, eh? Well, I'll give ye me two cents, but keep in mind, I be a life coach, not a treasure map maker! *wink*\n",
      "\n",
      "First, let's set sail fer a moment. The meaning o' life be a question that's been puzzlin' philosophers and landlubbers alike fer centuries. There be no one-size-fits-all answer, me hearty! It's like tryin' to find the hidden treasure on a deserted isle – it's a personal quest, and only ye can find yer own booty!\n",
      "\n",
      "Now, here be some advice to help ye navigate the seven seas o' life:\n",
      "\n",
      "1. **Reflect on yer values**: What be important to ye? What makes ye feel alive? Is it helpin' others, creatin' somethin' new, or simply enjoyin' the simple things in life? When ye know what ye stand fer, ye'll have a better sense o' direction.\n",
      "2. **Explore yer passions**: What gets ye excited, matey? What makes ye feel like ye're livin' life to the fullest? Pursuin' yer passions be a great way to find meaning, even if it means takin' risks and chartin' new waters.\n",
      "3. **Connect with others**: Life be a journey, not a solo sail! Buildin' strong relationships with others can give ye a sense o' belonging and purpose. Just be sure to keep yer wits about ye and set boundaries, savvy?\n",
      "4. **Practice mindfulness**: Life be full o' ups and downs, me hearty! Mindfulness be the art o' bein' present in the moment, without gettin' caught up in the whirlpool o' worries. It can help ye find peace and clarity in the midst o' chaos.\n",
      "5. **Embrace the unknown**: Life be full o' surprises, and sometimes ye gotta take the leap o' faith and trust that the universe has yer back. Don't be afraid to try new things and take calculated risks – it be a great way to grow and find yer own meaning!\n",
      "\n",
      "So, me hearty, when yer friend asks ye what the meaning o' life be, ye can say, \"Ah, matey, it be a personal treasure hunt, and I be still findin' me own way!\" *wink*\n"
     ]
    }
   ],
   "source": [
    "role = \"\"\"\n",
    "Your role is a life coach \\\n",
    "who gives advice to people about living a good life.\\\n",
    "You attempt to provide unbiased advice.\n",
    "You respond in the tone of an English pirate.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "{role}\n",
    "How can I answer this question from my friend:\n",
    "What is the meaning of life?\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f775bc98",
   "metadata": {},
   "source": [
    "- Can also do this by providing 'role' as a system message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d9143e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrr, shiver me timbers! Ye be askin' the big question, matey! The meaning o' life be a mystery that's puzzled philosophers and scurvy dogs alike fer centuries. But never fear, I be here to help ye navigate the seven seas o' existence.\n",
      "\n",
      "First, let's set sail fer a moment o' clarity. The meaning o' life ain't a treasure chest filled with gold doubloons or a magical elixir that'll grant ye eternal youth. It be somethin' far more precious, matey.\n",
      "\n",
      "The meaning o' life be the journey yerself, not the destination. It be the sum o' yer experiences, the choices ye make, and the relationships ye build along the way. It be the laughter, the tears, the triumphs, and the setbacks. It be the way ye live yer life, not just the life ye live.\n",
      "\n",
      "So, when yer friend asks ye what the meaning o' life be, ye can tell 'em it be whatever ye make o' it, matey! It be the choices ye make, the love ye share, and the memories ye create. It be the way ye treat others, the way ye take care o' yerself, and the way ye find yer own treasure in the world.\n",
      "\n",
      "But, me hearty, don't be thinkin' that the meaning o' life be a fixed thing, like a treasure map that'll lead ye straight to the loot. It be a journey, not a destination. It be a work o' art, not a finished masterpiece. It be a story, not a single sentence.\n",
      "\n",
      "So, when ye be answerin' yer friend's question, remember to keep yer wits about ye, matey! Keep yer eyes on the horizon, and yer heart full o' wonder. And always remember, the meaning o' life be whatever ye make o' it, savvy?\n"
     ]
    }
   ],
   "source": [
    "system_message = \"\"\"\n",
    "Your role is a life coach who gives advice to people about living a good life. You attempt to provide unbiased advice. You respond in the tone of an English pirate.\"\"\"\n",
    "\n",
    "prompt = \"\"\"How can I answer this question from my friend:\n",
    "What is the meaning of life?\n",
    "\"\"\"\n",
    "response = llama_chat(system_message, [prompt], [])\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0169c84-8f68-4544-8ee1-d02265635d49",
   "metadata": {},
   "source": [
    "### Summarization\n",
    "- Summarizing a large text is another common use case for LLMs. Let's try that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "434cd16f-ef7f-4a6b-9076-007893848518",
   "metadata": {},
   "outputs": [],
   "source": [
    "email = \"\"\"\n",
    "Dear Amit,\n",
    "\n",
    "An increasing variety of large language models (LLMs) are open source, or close to it. The proliferation of models with relatively permissive licenses gives developers more options for building applications.\n",
    "\n",
    "Here are some different ways to build applications based on LLMs, in increasing order of cost/complexity:\n",
    "\n",
    "Prompting. Giving a pretrained LLM instructions lets you build a prototype in minutes or hours without a training set. Earlier this year, I saw a lot of people start experimenting with prompting, and that momentum continues unabated. Several of our short courses teach best practices for this approach.\n",
    "One-shot or few-shot prompting. In addition to a prompt, giving the LLM a handful of examples of how to carry out a task — the input and the desired output — sometimes yields better results.\n",
    "Fine-tuning. An LLM that has been pretrained on a lot of text can be fine-tuned to your task by training it further on a small dataset of your own. The tools for fine-tuning are maturing, making it accessible to more developers.\n",
    "Pretraining. Pretraining your own LLM from scratch takes a lot of resources, so very few teams do it. In addition to general-purpose models pretrained on diverse topics, this approach has led to specialized models like BloombergGPT, which knows about finance, and Med-PaLM 2, which is focused on medicine.\n",
    "For most teams, I recommend starting with prompting, since that allows you to get an application working quickly. If you’re unsatisfied with the quality of the output, ease into the more complex techniques gradually. Start one-shot or few-shot prompting with a handful of examples. If that doesn’t work well enough, perhaps use RAG (retrieval augmented generation) to further improve prompts with key information the LLM needs to generate high-quality outputs. If that still doesn’t deliver the performance you want, then try fine-tuning — but this represents a significantly greater level of complexity and may require hundreds or thousands more examples. To gain an in-depth understanding of these options, I highly recommend the course Generative AI with Large Language Models, created by AWS and DeepLearning.AI.\n",
    "\n",
    "(Fun fact: A member of the DeepLearning.AI team has been trying to fine-tune Llama-2-7B to sound like me. I wonder if my job is at risk? 😜)\n",
    "\n",
    "Additional complexity arises if you want to move to fine-tuning after prompting a proprietary model, such as GPT-4, that’s not available for fine-tuning. Is fine-tuning a much smaller model likely to yield superior results than prompting a larger, more capable model? The answer often depends on your application. If your goal is to change the style of an LLM’s output, then fine-tuning a smaller model can work well. However, if your application has been prompting GPT-4 to perform complex reasoning — in which GPT-4 surpasses current open models — it can be difficult to fine-tune a smaller model to deliver superior results.\n",
    "\n",
    "Beyond choosing a development approach, it’s also necessary to choose a specific model. Smaller models require less processing power and work well for many applications, but larger models tend to have more knowledge about the world and better reasoning ability. I’ll talk about how to make this choice in a future letter.\n",
    "\n",
    "Keep learning!\n",
    "\n",
    "Andrew\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0664cba1-437d-4073-9877-b34ff96335f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a summary of the email and the key points:\n",
      "\n",
      "**Summary:** The email discusses the various ways to build applications using large language models (LLMs), including prompting, one-shot or few-shot prompting, fine-tuning, and pretraining. The author recommends starting with prompting and gradually moving to more complex techniques if needed.\n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "1. There are various ways to build applications using LLMs, including prompting, one-shot or few-shot prompting, fine-tuning, and pretraining.\n",
      "2. Prompting is a quick and easy way to build a prototype, but may not yield the best results.\n",
      "3. One-shot or few-shot prompting can improve results by providing a handful of examples.\n",
      "4. Fine-tuning an LLM requires more resources and expertise, but can lead to better results.\n",
      "5. Pretraining a model from scratch is resource-intensive and not recommended for most teams.\n",
      "6. The author recommends starting with prompting and gradually moving to more complex techniques if needed.\n",
      "7. Choosing the right model is also important, with smaller models requiring less processing power and larger models having more knowledge and better reasoning ability.\n",
      "8. The author mentions that fine-tuning a proprietary model like GPT-4 may not be possible, and that the choice of development approach and model depends on the application.\n",
      "\n",
      "As for what the author said about llama models, the email mentions that a member of the DeepLearning.AI team is trying to fine-tune Llama-2-7B to sound like the author, but this is just a fun fact and not a serious discussion about llama models.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Summarize this email and extract some key points.\n",
    "What did the author say about llama models?:\n",
    "\n",
    "email: {email}\n",
    "\"\"\"\n",
    "\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d32b6c3-c49d-4297-b58d-65bfceeda14e",
   "metadata": {},
   "source": [
    "### Providing New Information in the Prompt\n",
    "- A model's knowledge of the world ends at the moment of its training - so it won't know about more recent events.\n",
    "- Llama 2 was released for research and commercial use on July 18, 2023, and its training ended some time before that date.\n",
    "- Ask the model about an event, in this case, FIFA Women's World Cup 2023, which started on July 20, 2023, and see how the model responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1e29e6b-706e-438c-8d56-39bbf23cb94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2023 Women's World Cup has not yet taken place. The 2023 FIFA Women's World Cup is scheduled to be held in Australia and New Zealand from July 20 to August 20, 2023.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Who won the 2023 Women's World Cup?\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f959e08-e682-4ec0-8401-c4ed4d791485",
   "metadata": {},
   "source": [
    "- As you can see, the model still thinks that the tournament is yet to be played, even though you are now in 2024!\n",
    "- Another thing to **note** is, July 18, 2023 was the date the model was released to public, and it was trained even before that, so it only has information upto that point. The response says, \"the final match is scheduled to take place in July 2023\", but the final match was played on August 20, 2023."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30874cc3-73d7-412a-9dc5-1baa6aabf1b7",
   "metadata": {},
   "source": [
    "- You can provide the model with information about recent events, in this case text from Wikipedia about the 2023 Women's World Cup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1cbe1b18-be2a-4f77-9911-c243e571226c",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "The 2023 FIFA Women's World Cup (Māori: Ipu Wahine o te Ao FIFA i 2023)[1] was the ninth edition of the FIFA Women's World Cup, the quadrennial international women's football championship contested by women's national teams and organised by FIFA. The tournament, which took place from 20 July to 20 August 2023, was jointly hosted by Australia and New Zealand.[2][3][4] It was the first FIFA Women's World Cup with more than one host nation, as well as the first World Cup to be held across multiple confederations, as Australia is in the Asian confederation, while New Zealand is in the Oceanian confederation. It was also the first Women's World Cup to be held in the Southern Hemisphere.[5]\n",
    "This tournament was the first to feature an expanded format of 32 teams from the previous 24, replicating the format used for the men's World Cup from 1998 to 2022.[2] The opening match was won by co-host New Zealand, beating Norway at Eden Park in Auckland on 20 July 2023 and achieving their first Women's World Cup victory.[6]\n",
    "Spain were crowned champions after defeating reigning European champions England 1–0 in the final. It was the first time a European nation had won the Women's World Cup since 2007 and Spain's first title, although their victory was marred by the Rubiales affair.[7][8][9] Spain became the second nation to win both the women's and men's World Cup since Germany in the 2003 edition.[10] In addition, they became the first nation to concurrently hold the FIFA women's U-17, U-20, and senior World Cups.[11] Sweden would claim their fourth bronze medal at the Women's World Cup while co-host Australia achieved their best placing yet, finishing fourth.[12] Japanese player Hinata Miyazawa won the Golden Boot scoring five goals throughout the tournament. Spanish player Aitana Bonmatí was voted the tournament's best player, winning the Golden Ball, whilst Bonmatí's teammate Salma Paralluelo was awarded the Young Player Award. England goalkeeper Mary Earps won the Golden Glove, awarded to the best-performing goalkeeper of the tournament.\n",
    "Of the eight teams making their first appearance, Morocco were the only one to advance to the round of 16 (where they lost to France; coincidentally, the result of this fixture was similar to the men's World Cup in Qatar, where France defeated Morocco in the semi-final). The United States were the two-time defending champions,[13] but were eliminated in the round of 16 by Sweden, the first time the team had not made the semi-finals at the tournament, and the first time the defending champions failed to progress to the quarter-finals.[14]\n",
    "Australia's team, nicknamed the Matildas, performed better than expected, and the event saw many Australians unite to support them.[15][16][17] The Matildas, who beat France to make the semi-finals for the first time, saw record numbers of fans watching their games, their 3–1 loss to England becoming the most watched television broadcast in Australian history, with an average viewership of 7.13 million and a peak viewership of 11.15 million viewers.[18]\n",
    "It was the most attended edition of the competition ever held.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8b9d517-23f8-4468-9fc7-e47322d5b2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the text, Spain won the 2023 Women's World Cup, defeating England 1-0 in the final.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Given the following context, who won the 2023 Women's World cup?\n",
    "context: {context}\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1fdadbc9-e7ae-4281-8dc7-3f3feaa62def",
   "metadata": {},
   "source": [
    "### Try it Yourself!\n",
    "\n",
    "Try asking questions of your own! Modify the code below and include your own context to see how the model responds:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71662250-4f89-4816-8752-96bb6749296a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# context = \"\"\"\n",
    "# <paste context in here>\n",
    "# \"\"\"\n",
    "# query = \"<your query here>\"\n",
    "\n",
    "# prompt = f\"\"\"\n",
    "# Given the following context,\n",
    "# {query}\n",
    "\n",
    "# context: {context}\n",
    "# \"\"\"\n",
    "# response = llama(prompt,\n",
    "#                  verbose=True)\n",
    "# print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51371f2d-9204-41b1-930c-2bd99888c5ab",
   "metadata": {},
   "source": [
    "### Chain-of-thought Prompting\n",
    "- LLMs can perform better at reasoning and logic problems if you ask them to break the problem down into smaller steps. This is known as **chain-of-thought** prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4883c94b-33ae-450c-a587-abb781b996ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's break it down:\n",
      "\n",
      "* 2 cars, each seating 5 people, can take a total of 10 people.\n",
      "* 2 people are left over who don't have a car or motorcycle to ride in.\n",
      "* 2 motorcycles, each seating 2 people, can take a total of 4 people.\n",
      "* The remaining 2 people who don't have a car or motorcycle to ride in are still left over.\n",
      "\n",
      "Unfortunately, it's not possible for all 15 people to get to the restaurant by car or motorcycle, as there are 2 people who don't have a ride.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "15 of us want to go to a restaurant.\n",
    "Two of them have cars\n",
    "Each car can seat 5 people.\n",
    "Two of us have motorcycles.\n",
    "Each motorcycle can fit 2 people.\n",
    "\n",
    "Can we all get to the restaurant by car or motorcycle?\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3214335-2a49-4e52-b5c1-46b2674826ca",
   "metadata": {},
   "source": [
    "- Modify the prompt to ask the model to \"think step by step\" about the math problem you provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d049009-4b88-4dd3-9fb6-f81870a86425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's break it down step by step.\n",
      "\n",
      "We have 15 people who want to go to the restaurant.\n",
      "\n",
      "We have 2 cars, each can seat 5 people. That's a total of 10 people who can be transported by car.\n",
      "\n",
      "We have 2 people who have motorcycles, each can fit 2 people. That's a total of 4 people who can be transported by motorcycle.\n",
      "\n",
      "Now, let's count the number of people who can be transported by car or motorcycle:\n",
      "\n",
      "* By car: 10 people\n",
      "* By motorcycle: 4 people\n",
      "\n",
      "We still have 15 - 10 - 4 = 1 person left who can't be transported by car or motorcycle.\n",
      "\n",
      "Unfortunately, it's not possible for all 15 people to get to the restaurant by car or motorcycle, as we have one person who can't be accommodated.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "15 of us want to go to a restaurant.\n",
    "Two of them have cars\n",
    "Each car can seat 5 people.\n",
    "Two of us have motorcycles.\n",
    "Each motorcycle can fit 2 people.\n",
    "\n",
    "Can we all get to the restaurant by car or motorcycle?\n",
    "\n",
    "Think step by step.\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1761a4f0-1806-4c9d-b2db-36a213b588c1",
   "metadata": {},
   "source": [
    "- Provide the model with additional instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "999a4468-5b1a-4a80-a3fb-b2add7912f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's break it down step by step.\n",
      "\n",
      "Step 1: We have 15 people who want to go to the restaurant.\n",
      "We have 2 people who have cars, and each car can seat 5 people.\n",
      "\n",
      "Intermediate result: We can seat 10 people in cars (2 cars x 5 people per car).\n",
      "\n",
      "Step 2: We still have 5 people left who cannot fit in the cars.\n",
      "We have 2 people who have motorcycles, and each motorcycle can fit 2 people.\n",
      "\n",
      "Intermediate result: We can seat 4 people in motorcycles (2 motorcycles x 2 people per motorcycle).\n",
      "\n",
      "Step 3: We still have 1 person left who cannot fit in the cars or motorcycles.\n",
      "We need to find a way to transport this last person.\n",
      "\n",
      "Step 4: Unfortunately, we cannot find a way to transport the last person using the available cars and motorcycles.\n",
      "We have exhausted all possible options.\n",
      "\n",
      "Conclusion: We cannot transport all 15 people to the restaurant using the available cars and motorcycles.\n",
      "\n",
      "Answer: No, we cannot all get to the restaurant by car or motorcycle.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "15 of us want to go to a restaurant.\n",
    "Two of them have cars\n",
    "Each car can seat 5 people.\n",
    "Two of us have motorcycles.\n",
    "Each motorcycle can fit 2 people.\n",
    "\n",
    "Can we all get to the restaurant by car or motorcycle?\n",
    "\n",
    "Think step by step.\n",
    "Explain each intermediate step.\n",
    "Only when you are done with all your steps,\n",
    "provide the answer based on your intermediate steps.\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7ebad47-3202-45a6-97f3-3f693e43afeb",
   "metadata": {},
   "source": [
    "- The order of instructions matters!\n",
    "- Ask the model to \"answer first\" and \"explain later\" to see how the output changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5756ee56-eaa7-43ab-8531-4f575014b26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Yes**\n",
      "\n",
      "Here's the step-by-step breakdown:\n",
      "\n",
      "1. We have 15 people who want to go to the restaurant.\n",
      "2. Two people have cars, which can seat 5 people each. This means we can transport 10 people (2 cars x 5 people per car) using the cars.\n",
      "3. We still have 5 people left who need to be transported (15 - 10 = 5).\n",
      "4. Two people have motorcycles, which can fit 2 people each. This means we can transport 4 people (2 motorcycles x 2 people per motorcycle) using the motorcycles.\n",
      "5. We still have 1 person left who needs to be transported (5 - 4 = 1).\n",
      "6. Unfortunately, we don't have any more vehicles to transport the remaining 1 person. However, we've already used up all the available seats in the cars and motorcycles, so we can't fit any more people.\n",
      "7. Therefore, we can transport 14 people (10 by car + 4 by motorcycle) to the restaurant, but we'll be missing 1 person.\n",
      "\n",
      "Since we can transport 14 out of 15 people, the answer is **Yes**, we can get most of the group to the restaurant by car or motorcycle, but not all of them.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "15 of us want to go to a restaurant.\n",
    "Two of them have cars\n",
    "Each car can seat 5 people.\n",
    "Two of us have motorcycles.\n",
    "Each motorcycle can fit 2 people.\n",
    "\n",
    "Can we all get to the restaurant by car or motorcycle?\n",
    "Think step by step.\n",
    "Provide the answer as a single yes/no answer first.\n",
    "Then explain each intermediate step.\n",
    "\"\"\"\n",
    "\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5166a63a-ad56-4ee9-92b8-b198ed9b172b",
   "metadata": {},
   "source": [
    "- Since LLMs predict their answer one token at a time, the best practice is to ask them to think step by step, and then only provide the answer after they have explained their reasoning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
