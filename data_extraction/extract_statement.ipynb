{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pdfplumber\n",
    "import re\n",
    "\n",
    "from utils import create_new_sheet, write_content, format_sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Update the following parameters ##################\n",
    "FILEPATH = \"../data/June statement.pdf\"\n",
    "NEW_SHEET_NAME = \"June 2024\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPREADSHEET_ID = \"1Ik2lyOO5MN9a8-hEUf4sRqUlP3WcN36ItekQSGV4NxY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_lines = []\n",
    "with pdfplumber.open(FILEPATH) as pdf:\n",
    "    for page_id in range(len(pdf.pages)):\n",
    "        page = pdf.pages[page_id]\n",
    "        pdf_lines.extend(page.extract_text_lines())\n",
    "pdf_lines = [line['text'] for line in pdf_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "# lr_model = Pipeline([('vect', CountVectorizer()),\n",
    "#                ('tfidf', TfidfTransformer()),\n",
    "#                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "#               ])\n",
    "\n",
    "lr_model = pickle.load(open('pickle/lr_model.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_label = {0: 'Regular', 1: 'Trip', 2: 'Hobby'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_pattern = \"^(\\d\\d/\\d\\d.*) (-*[\\d\\.,]+)$\"\n",
    "accumulated_content = []\n",
    "for line in pdf_lines:\n",
    "    if \"AUTOMATIC PAYMENT\" in line:\n",
    "        continue\n",
    "    re_search = re.search(regex_pattern, line)\n",
    "    if re_search is not None:\n",
    "        transaction_name = re_search.group(1)\n",
    "        expense_type_id = lr_model.predict([transaction_name])[0]\n",
    "        expense_type = id_to_label[expense_type_id] if expense_type_id in id_to_label else 'Regular'\n",
    "        cost = float(re_search.group(2).replace(',', ''))\n",
    "        accumulated_content.append([transaction_name, cost, expense_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the content according to the transaction date\n",
    "accumulated_content = sorted(accumulated_content, key=lambda x: x[0].split(' ')[0])\n",
    "\n",
    "for i in range(len(accumulated_content)):\n",
    "    row_id = i + 2\n",
    "    regular_cost = f'=IF(C{row_id}=\"Regular\", B{row_id}, 0)'\n",
    "    trip_cost = f'=IF(C{row_id}=\"Trip\", B{row_id}, 0)'\n",
    "    hobby_cost = f'=IF(C{row_id}=\"Hobby\", B{row_id}, 0)'\n",
    "    accumulated_content[i].append(regular_cost)\n",
    "    accumulated_content[i].append(trip_cost)\n",
    "    accumulated_content[i].append(hobby_cost)\n",
    "\n",
    "accumulated_content = [['Transaction', 'Cost', 'Type', 'Regular expenses', 'Trip expenses', 'Hobby expenses']] + accumulated_content + [['', f'=SUM(B2:B{row_id})', '', f'=SUM(D2:D{row_id})', f'=SUM(E2:E{row_id})', f'=SUM(F2:F{row_id})', f'=SUM(D{row_id+1}:F{row_id+1})']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the content according to the transaction date\n",
    "# accumulated_content = [accumulated_content[0]] + sorted(accumulated_content[1:-1], key=lambda x: x[0].split(' ')[0]) + [accumulated_content[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a new sheet result: {'spreadsheetId': '1Ik2lyOO5MN9a8-hEUf4sRqUlP3WcN36ItekQSGV4NxY', 'replies': [{'addSheet': {'properties': {'sheetId': 1203295335, 'title': 'June 2024', 'index': 12, 'sheetType': 'GRID', 'gridProperties': {'rowCount': 1000, 'columnCount': 26}}}}]}\n",
      "Exported content to 433 cells.\n",
      "Format sheet result: {'spreadsheetId': '1Ik2lyOO5MN9a8-hEUf4sRqUlP3WcN36ItekQSGV4NxY', 'replies': [{}, {}, {}]}\n"
     ]
    }
   ],
   "source": [
    "# Export to the spreadsheet.\n",
    "create_new_sheet_response = create_new_sheet(SPREADSHEET_ID, NEW_SHEET_NAME)\n",
    "sheet_id = create_new_sheet_response['replies'][0]['addSheet']['properties']['sheetId']\n",
    "write_response = write_content(SPREADSHEET_ID, NEW_SHEET_NAME, accumulated_content)\n",
    "format_sheet_response = format_sheet(SPREADSHEET_ID, sheet_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
