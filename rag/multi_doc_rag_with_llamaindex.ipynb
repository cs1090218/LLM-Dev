{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG over multiple documents\n",
    "TODO\n",
    "- Add more documents\n",
    "- Maybe use summary tool to auto-generate summaries\n",
    "- Persist for every doc the index as well as the generated summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import nest_asyncio\n",
    "from utils.gdrive_api import download_gfile\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_OPENAI = True\n",
    "\n",
    "CHUNK_SIZE = 512\n",
    "CHUNK_OVERLAP = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download files from Drive - Uncomment and add filenames as required\n",
    "# filenames = [\n",
    "#     'Trippy Trip']\n",
    "# for filename in filenames:\n",
    "#     output_path = \"../data/\" + (filename if filename.endswith('.pdf') else filename + '.pdf')\n",
    "#     download_gfile(filename, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "if USE_OPENAI:\n",
    "    Settings.llm = OpenAI(model=\"gpt-3.5-turbo\", api_key=os.getenv('OPENAI_API_KEY'))\n",
    "    Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")\n",
    "else:\n",
    "    Settings.llm = Ollama(model=\"llama3:instruct\", request_timeout=120.0)\n",
    "    Settings.embed_model = OllamaEmbedding(\n",
    "        model_name=\"llama3:instruct\",\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        ollama_additional_kwargs={\"mirostat\": 0})\n",
    "\n",
    "Settings.node_parser = SentenceSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = [\n",
    "    \"idpp.pdf\", \"metagpt.pdf\", \"state_of_the_union.txt\", \"Federal Tax Return 2021.pdf\", \"Financial Assessment.docx\",\n",
    "    \"IELTS Result - March 2023.pdf\", \"Shashank Verma - Resume.pdf\", 'Shashank Verma - Resume (2024).pdf', 'LLM Pointers.pdf',\n",
    "    'Multi-class classification via Transfer Learning for classifying dog breeds from images.pdf',\n",
    "    'Incorporating adaptive feedback capability to Interactive Videos tutor in Project Ivy.pdf', 'Spring 2024 Reflection Report - Shashank.pdf',\n",
    "    'Shashank - Medical Log.pdf', 'Pranjali - Medical Log.pdf', 'Shashank - 2023-06-16 Lab Results.pdf', 'Pranjali - 2023-06-16 Lab Results.pdf',\n",
    "    'Trippy Trip.pdf']\n",
    "input_files = [\"../data/\" + item for item in input_files]\n",
    "\n",
    "file_summaries = [\n",
    "    \"Useful for retrieving specific context from the iDPP paper which is about predicting ALSFRS-R rating scores for ALS patients.\",\n",
    "    \"Useful for retrieving specific context from the MetaGPT paper.\",\n",
    "    \"Useful for retrieving specific context from the state of the union speech by the president.\",\n",
    "    \"Useful for retrieving specific context from the Federal Tax Return of 2021 detailing things like gross income, taxable income, tax paid and so on\",\n",
    "    \"Useful for retrieving specific context from Financial Assessment detailing how much to spend per month on various things\",\n",
    "    \"Useful for retrieving specific context from my IELTS result I got in 2023 denoting my English speaking skills\",\n",
    "    \"Useful for retrieving specific context from my resume of 2023 specifying what projects I've worked on, where I studied, what my qualifications are, etc\",\n",
    "    \"Useful for retrieving specific context from my resume of 2024 specifying what projects I've worked on, where I studied, what my qualifications are, etc\",\n",
    "    \"Useful for retrieving specific context from some pointers I've gathered regarding LLMs\",\n",
    "    \"Useful for retrieving specific context from a project I did where I had to do multi-class classification via transfer learning for classifying dog breeds from images\",\n",
    "    \"Useful for retrieving specific context from my project proposal for 'Ivy' master's project with Professor Ashok Goel where we want to build an interactive tutor for incorporating adaptive feedback\",\n",
    "    \"Useful for retrieving specific context from my reflection report at the end of Spring semester in 2024 for Project 'Ivy' - my master's project\",\n",
    "    \"Useful for retrieving specific context from my log of medical tests and conditions over the years\",\n",
    "    \"Useful for retrieving specific context from Pranjali's log of medical tests and conditions over the years\",\n",
    "    \"Useful for retrieving specific context from my bloodwork lab results from 2023\",\n",
    "    \"Useful for retrieving specific context from Pranjali's bloodwork lab results from 2023\",\n",
    "    \"Useful for retrieving specific context from all our trips that Pranjali and myself have taken over the years including trip plans and how much it cost on those trips\",\n",
    "]\n",
    "assert len(input_files) == len(file_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving vector_index for idpp from storage\n",
      "Retrieving vector_index for metagpt from storage\n",
      "Retrieving vector_index for state_of_the_union from storage\n",
      "Retrieving vector_index for Federal Tax Return 2021 from storage\n",
      "Retrieving vector_index for Financial Assessment from storage\n",
      "Retrieving vector_index for IELTS Result - March 2023 from storage\n",
      "Retrieving vector_index for Shashank Verma - Resume from storage\n",
      "Retrieving vector_index for Shashank Verma - Resume (2024) from storage\n",
      "Retrieving vector_index for LLM Pointers from storage\n",
      "Retrieving vector_index for Multi-class classification via Transfer Learning for classifying dog breeds from images from storage\n",
      "Retrieving vector_index for Incorporating adaptive feedback capability to Interactive Videos tutor in Project Ivy from storage\n",
      "Retrieving vector_index for Spring 2024 Reflection Report - Shashank from storage\n",
      "Retrieving vector_index for Shashank - Medical Log from storage\n",
      "Retrieving vector_index for Pranjali - Medical Log from storage\n",
      "Retrieving vector_index for Shashank - 2023-06-16 Lab Results from storage\n",
      "Retrieving vector_index for Pranjali - 2023-06-16 Lab Results from storage\n",
      "Creating vector_index for Trippy Trip\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "vector_indices = []\n",
    "for file in input_files:\n",
    "    filename = os.path.basename(file).split('.')[0]\n",
    "    # Read from storage if using OPENAI and the embeddings exist\n",
    "    if USE_OPENAI and os.path.exists(f\"file_embeddings/openAI/{filename}\"):\n",
    "        print (f\"Retrieving vector_index for {filename} from storage\")\n",
    "        storage_context = StorageContext.from_defaults(persist_dir=f\"file_embeddings/openAI/{filename}\")\n",
    "        vector_index = load_index_from_storage(storage_context=storage_context)\n",
    "    # Otherwise create index using regular method\n",
    "    else:\n",
    "        print (f\"Creating vector_index for {filename}\")\n",
    "        loader = SimpleDirectoryReader(input_files=[file])\n",
    "        documents = loader.load_data()\n",
    "\n",
    "        # splitter = Settings.node_parser\n",
    "        # nodes = splitter.get_nodes_from_documents(documents)\n",
    "        # index = VectorStoreIndex(nodes)\n",
    "        vector_index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "    vector_indices.append(vector_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving vector_index for idpp\n",
      "Saving vector_index for metagpt\n",
      "Saving vector_index for state_of_the_union\n",
      "Saving vector_index for Federal Tax Return 2021\n",
      "Saving vector_index for Financial Assessment\n",
      "Saving vector_index for IELTS Result - March 2023\n",
      "Saving vector_index for Shashank Verma - Resume\n",
      "Saving vector_index for Shashank Verma - Resume (2024)\n",
      "Saving vector_index for LLM Pointers\n",
      "Saving vector_index for Multi-class classification via Transfer Learning for classifying dog breeds from images\n",
      "Saving vector_index for Incorporating adaptive feedback capability to Interactive Videos tutor in Project Ivy\n",
      "Saving vector_index for Spring 2024 Reflection Report - Shashank\n",
      "Saving vector_index for Shashank - Medical Log\n",
      "Saving vector_index for Pranjali - Medical Log\n",
      "Saving vector_index for Shashank - 2023-06-16 Lab Results\n",
      "Saving vector_index for Pranjali - 2023-06-16 Lab Results\n",
      "Saving vector_index for Trippy Trip\n"
     ]
    }
   ],
   "source": [
    "if USE_OPENAI:\n",
    "    for i in range(len(input_files)):\n",
    "        filename = os.path.basename(input_files[i]).split('.')[0]\n",
    "        print (f\"Saving vector_index for {filename}\")\n",
    "        vector_indices[i].storage_context.persist(f\"file_embeddings/openAI/{filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "query_engine_tools = []\n",
    "for i in range(len(input_files)):\n",
    "    query_engine_tools.append(QueryEngineTool.from_defaults(\n",
    "        query_engine=vector_indices[i].as_query_engine(),\n",
    "        description=file_summaries[i],\n",
    "    ))\n",
    "\n",
    "len(query_engine_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Using ToolRetrieverRouterQueryEngine\n",
    "# from llama_index.core import VectorStoreIndex\n",
    "# from llama_index.core.objects import ObjectIndex\n",
    "# from llama_index.core.query_engine import ToolRetrieverRouterQueryEngine\n",
    "\n",
    "# obj_index = ObjectIndex.from_objects(query_engine_tools, index_cls=VectorStoreIndex)\n",
    "# query_engine = ToolRetrieverRouterQueryEngine(obj_index.as_retriever())\n",
    "\n",
    "# response = query_engine.query(\"What do the authors say in iDPP paper\")\n",
    "# print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using RouterQueryEngine\n",
    "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(),\n",
    "    query_engine_tools=query_engine_tools,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 16: This choice is relevant as it pertains to all the trips taken over the years, including trip plans and costs. By referring to this choice, you can retrieve specific context about the trips taken in 2017..\n",
      "\u001b[0mEuro 2.0\n"
     ]
    }
   ],
   "source": [
    "query = \"Where did we go in 2017?\"\n",
    "respones = query_engine.query(query)\n",
    "print (respones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Questions\n",
    "- What do the authors say in iDPP paper\n",
    "- What were the models tried for predicting ALS progression in the idpp paper?\n",
    "- What was the validation strategy used by the authors in the idpp paper?\n",
    "- Which model performed the best with lowest RMSE in the idpp paper?\n",
    "- What did the president say about Justice Breyer?\n",
    "- How do MetaGPT agents share information with other agents?\n",
    "- What was my reading score in IELTS exam?\n",
    "- What is my educational background according to my resume?\n",
    "- How much do I expect to spend per month on Hobbies?\n",
    "- What was my total tax amount to be paid in 2021?\n",
    "- What was my total gross income in 2021?\n",
    "- What is my OpenAI secret key?\n",
    "- What were my deliverables in Ivy?\n",
    "- Are LLMs free to use?\n",
    "- Describe my methodology for classifying dog breeds project.\n",
    "- Give me my medical history.\n",
    "- What are some of the notable projects I've worked on in my professional career?\n",
    "- What is Pranjali's medical history?\n",
    "- What is the summary of my latest bloodwork?\n",
    "- Where did we go in 2017?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
