{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b523e0a",
   "metadata": {},
   "source": [
    "# Lesson 4: Building a Multi-Document Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a323703",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9625ab2-71b6-4fd0-904e-42df80d3215f",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "USE_OPENAI = False  # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3221a474-5817-4db2-af46-e029042a75a5",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65b9ae19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "if USE_OPENAI:\n",
    "    Settings.llm = OpenAI(model=\"gpt-3.5-turbo\", api_key=os.getenv('OPENAI_API_KEY'))\n",
    "    Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")\n",
    "else:\n",
    "    Settings.llm = Ollama(model=\"llama3:instruct\", request_timeout=120.0)\n",
    "    Settings.embed_model = OllamaEmbedding(\n",
    "        model_name=\"llama3:instruct\",\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        ollama_additional_kwargs={\"mirostat\": 0})\n",
    "\n",
    "Settings.node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20adaa26",
   "metadata": {},
   "source": [
    "## 1. Setup an agent over 3 papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b71ff6",
   "metadata": {},
   "source": [
    "**Note**: The pdf files are included with this lesson. To access these papers, go to the `File` menu and select`Open...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed10a24b-d65c-4b98-a93a-94ccdb8900d0",
   "metadata": {
    "height": 200,
    "tags": []
   },
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"../data/metagpt.pdf\",\n",
    "    \"../data/longlora.pdf\",\n",
    "    \"../data/selfrag.pdf\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d8f3185-3221-4b00-bd38-41d36e4a3307",
   "metadata": {
    "height": 149,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: ../data/metagpt.pdf\n",
      "Getting tools for paper: ../data/longlora.pdf\n",
      "Getting tools for paper: ../data/selfrag.pdf\n"
     ]
    }
   ],
   "source": [
    "from utils import get_doc_tools\n",
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e541bdd-14e1-41b6-81b5-b1bfda078d07",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bff58c52",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "llm = Settings.llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f2c6a9f",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(initial_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a124a438-5609-402e-8642-69d1088cb9ad",
   "metadata": {
    "height": 166,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    initial_tools, \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17409d4c-05a9-4bf4-b74f-75135fa3cb6b",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in LongLoRA, and then tell me about the evaluation results\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation dataset\"}\n",
      "=== Function Output ===\n",
      "PG19\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation results\"}\n",
      "=== Function Output ===\n",
      "The evaluation results include reporting perplexity for models and baselines on proof-pile (Azerbayev et al., 2022) and PG19 datasets. The models achieve better perplexity with longer context sizes, indicating the effectiveness of the fine-tuning method. The perplexity decreases as the context size increases, with improvements observed when increasing the context window size. Additionally, the maximum context length that can be fine-tuned on a single 8 Ã— A100 machine is examined, showing promising results on extremely large settings. Some perplexity degradation is noted on small context sizes for extended models, which is a known limitation of Position Interpolation. Experiments on retrieval in long contexts are also conducted, comparing the model with other open LLMs on the topic retrieval task introduced in LongChat.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in LongLoRA includes the proof-pile (Azerbayev et al., 2022) and PG19 datasets. The evaluation results reported perplexity for models and baselines on these datasets. The models achieved better perplexity with longer context sizes, indicating the effectiveness of the fine-tuning method. Perplexity decreased as the context size increased, with improvements observed when increasing the context window size. The maximum context length that can be fine-tuned on a single 8 x A100 machine was examined, showing promising results on extremely large settings. Some perplexity degradation was noted on small context sizes for extended models, which is a known limitation of Position Interpolation. Experiments on retrieval in long contexts were also conducted, comparing the model with other open LLMs on the topic retrieval task introduced in LongChat.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used in LongLoRA, \"\n",
    "    \"and then tell me about the evaluation results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ace340b1-761f-4058-be41-68cf131541e4",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Give me a summary of both Self-RAG and LongLoRA\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_selfrag with args: {\"input\": \"Self-RAG\"}\n",
      "=== Function Output ===\n",
      "Self-RAG is a framework that enhances the quality and factuality of large language models by training a language model to learn to retrieve, generate, and critique text passages and its own generation through the use of reflection tokens. It involves a Critic LM and a Generator LM to evaluate text based on these reflection tokens, predicting whether external information retrieval is necessary and generating responses accordingly. The system aims to ensure that responses are relevant, supported by evidence, and useful in answering queries, with human evaluations indicating that Self-RAG outputs are often plausible, supported by relevant passages, and aligned with the predicted reflection tokens. The system's performance can be impacted by the scale of training data and the accuracy of predicting reflection tokens.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA\"}\n",
      "=== Function Output ===\n",
      "LongLoRA is a framework designed to efficiently extend the context sizes of pre-trained large language models by introducing Shifted Sparse Attention (S2-Attn) to approximate standard self-attention patterns during training. This method allows for significantly increasing the context length of language models while reducing GPU memory cost and training time compared to standard full fine-tuning approaches. Additionally, LongLoRA incorporates components like Flash-Attention2 and trainable normalization and embedding layers to bridge the gap between LoRA and full fine-tuning methods, aiming to enhance model performance on long-context benchmarks while maintaining computational efficiency.\n",
      "=== LLM Response ===\n",
      "Here are summaries of Self-RAG and LongLoRA:\n",
      "\n",
      "1. Self-RAG is a framework that enhances the quality and factuality of large language models by training a language model to learn to retrieve, generate, and critique text passages and its own generation through the use of reflection tokens. It involves a Critic LM and a Generator LM to evaluate text based on these reflection tokens, predicting whether external information retrieval is necessary and generating responses accordingly. The system aims to ensure that responses are relevant, supported by evidence, and useful in answering queries, with human evaluations indicating that Self-RAG outputs are often plausible, supported by relevant passages, and aligned with the predicted reflection tokens. The system's performance can be impacted by the scale of training data and the accuracy of predicting reflection tokens.\n",
      "\n",
      "2. LongLoRA is a framework designed to efficiently extend the context sizes of pre-trained large language models by introducing Shifted Sparse Attention (S2-Attn) to approximate standard self-attention patterns during training. This method allows for significantly increasing the context length of language models while reducing GPU memory cost and training time compared to standard full fine-tuning approaches. Additionally, LongLoRA incorporates components like Flash-Attention2 and trainable normalization and embedding layers to bridge the gap between LoRA and full fine-tuning methods, aiming to enhance model performance on long-context benchmarks while maintaining computational efficiency.\n",
      "Here are summaries of Self-RAG and LongLoRA:\n",
      "\n",
      "1. Self-RAG is a framework that enhances the quality and factuality of large language models by training a language model to learn to retrieve, generate, and critique text passages and its own generation through the use of reflection tokens. It involves a Critic LM and a Generator LM to evaluate text based on these reflection tokens, predicting whether external information retrieval is necessary and generating responses accordingly. The system aims to ensure that responses are relevant, supported by evidence, and useful in answering queries, with human evaluations indicating that Self-RAG outputs are often plausible, supported by relevant passages, and aligned with the predicted reflection tokens. The system's performance can be impacted by the scale of training data and the accuracy of predicting reflection tokens.\n",
      "\n",
      "2. LongLoRA is a framework designed to efficiently extend the context sizes of pre-trained large language models by introducing Shifted Sparse Attention (S2-Attn) to approximate standard self-attention patterns during training. This method allows for significantly increasing the context length of language models while reducing GPU memory cost and training time compared to standard full fine-tuning approaches. Additionally, LongLoRA incorporates components like Flash-Attention2 and trainable normalization and embedding layers to bridge the gap between LoRA and full fine-tuning methods, aiming to enhance model performance on long-context benchmarks while maintaining computational efficiency.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\"Give me a summary of both Self-RAG and LongLoRA\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eede70c",
   "metadata": {},
   "source": [
    "## 2. Setup an agent over 11 papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18771e69",
   "metadata": {},
   "source": [
    "### Download 11 ICLR papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60d01d2c-547f-4054-b0fe-ed9b1a9cc3b5",
   "metadata": {
    "height": 472,
    "tags": []
   },
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=LzPWWPAdY4\",\n",
    "    \"https://openreview.net/pdf?id=VTF8yNQM66\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "    \"https://openreview.net/pdf?id=9WD9KwssyT\",\n",
    "    \"https://openreview.net/pdf?id=yV6fD7LYkF\",\n",
    "    \"https://openreview.net/pdf?id=hnrB5YHoYu\",\n",
    "    \"https://openreview.net/pdf?id=WbWtOYIzIK\",\n",
    "    \"https://openreview.net/pdf?id=c5pwL0Soay\",\n",
    "    \"https://openreview.net/pdf?id=TpD2aG1h0D\"\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"selfrag.pdf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77426cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "To download these papers, below is the needed code:\n",
    "\n",
    "\n",
    "    #for url, paper in zip(urls, papers):\n",
    "         #!wget \"{url}\" -O \"{paper}\"\n",
    "    \n",
    "    \n",
    "**Note**: The pdf files are included with this lesson. To access these papers, go to the `File` menu and select`Open...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea5ee34d-02ac-4537-ae20-7ef6c5767172",
   "metadata": {
    "height": 149,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from utils import get_doc_tools\n",
    "# from pathlib import Path\n",
    "\n",
    "# paper_to_tools_dict = {}\n",
    "# for paper in papers:\n",
    "#     print(f\"Getting tools for paper: {paper}\")\n",
    "#     vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "#     paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e35d52c",
   "metadata": {},
   "source": [
    "### Extend the Agent with Tool Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20154923-873e-4941-9a3a-4926ab5f9b8c",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_tools = initial_tools  # [t for paper in papers for t in paper_to_tools_dict[paper]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "671582f9-70d7-4a8f-b813-58b2a068ca72",
   "metadata": {
    "height": 149,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define an \"object\" index and retriever over these tools\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    all_tools,\n",
    "    index_cls=VectorStoreIndex,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3929882-e9dc-46ca-b495-53e3ed60340e",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "obj_retriever = obj_index.as_retriever(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba9cfecd-fe14-4da8-b9ba-b3d485d98a03",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tools = obj_retriever.retrieve(\n",
    "    \"Tell me about the eval dataset used in MetaGPT and SWE-Bench\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c270ffbf-69c7-48ea-a028-9ba25221cde5",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMetadata(description='Useful for summarization questions related to longlora', name='summary_tool_longlora', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools[2].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9cc0a0b6-9858-4348-9ae0-1cd4160f3fb7",
   "metadata": {
    "height": 251,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tool_retriever=obj_retriever,\n",
    "    llm=llm, \n",
    "    system_prompt=\"\"\" \\\n",
    "You are an agent designed to answer queries over a set of given papers.\n",
    "Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
    "\n",
    "\"\"\",\n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a250cf1a-e011-4994-bcca-4e0294f20864",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in MetaGPT and compare it against SWE-Bench\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metagpt with args: {\"input\": \"evaluation dataset used in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in MetaGPT is the SoftwareDev dataset, which consists of 70 diverse software development tasks. These tasks range from creating Python GUI apps for drawing images, implementing color meters, and developing games like Snake, Brick breaker, 2048, Flappy bird, and Tank battle, to tasks related to Excel data processing, CRUD management, music transcription, custom press releases, Gomoku game, and weather dashboard development.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_selfrag with args: {\"input\": \"evaluation dataset used in SWE-Bench\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in SWE-Bench is not explicitly mentioned in the provided context information.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in MetaGPT is the SoftwareDev dataset, which consists of 70 diverse software development tasks. These tasks range from creating Python GUI apps for drawing images, implementing color meters, and developing games like Snake, Brick breaker, 2048, Flappy bird, and Tank battle, to tasks related to Excel data processing, CRUD management, music transcription, custom press releases, Gomoku game, and weather dashboard development.\n",
      "\n",
      "The evaluation dataset used in SWE-Bench is not explicitly mentioned in the provided context information.\n",
      "The evaluation dataset used in MetaGPT is the SoftwareDev dataset, which consists of 70 diverse software development tasks. These tasks range from creating Python GUI apps for drawing images, implementing color meters, and developing games like Snake, Brick breaker, 2048, Flappy bird, and Tank battle, to tasks related to Excel data processing, CRUD management, music transcription, custom press releases, Gomoku game, and weather dashboard development.\n",
      "\n",
      "The evaluation dataset used in SWE-Bench is not explicitly mentioned in the provided context information.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used \"\n",
    "    \"in MetaGPT and compare it against SWE-Bench\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8084c8cb-98ed-4835-aaa4-5b0c7254be6d",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Compare and contrast the LoRA papers (LongLoRA, LoftQ). Analyze the approach in each paper first. \n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA paper\"}\n",
      "=== Function Output ===\n",
      "The LongLoRA paper introduces an efficient fine-tuning approach for extending the context length of large language models. It utilizes Shifted Sparse Attention (S2-Attn) during training to approximate standard self-attention patterns, enabling the extension of context lengths with reduced GPU memory cost and training time compared to full fine-tuning. LongLoRA combines trainable normalization and embedding layers to bridge the gap between low-rank adaptation (LoRA) and full fine-tuning, demonstrating its effectiveness in extending context lengths for improved performance, particularly in question-answering tasks.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"LoftQ paper\"}\n",
      "=== Function Output ===\n",
      "The LoftQ paper introduces an Action Units Relation Learning framework that includes the Action Units Relation Transformer (ART) and the Tampered AU Prediction (TAP). It focuses on modeling relations between facial action units, generating challenging pseudo-samples for model learning, achieving top performance on evaluations, and offering visualizations of tampered regions. The key contributions are the ART encoder for intra-face relations and the TAP process for creating difficult pseudo-samples to enhance model generalization.\n",
      "=== LLM Response ===\n",
      "The LongLoRA paper introduces an efficient fine-tuning approach for extending the context length of large language models. It utilizes Shifted Sparse Attention (S2-Attn) during training to approximate standard self-attention patterns, enabling the extension of context lengths with reduced GPU memory cost and training time compared to full fine-tuning. LongLoRA combines trainable normalization and embedding layers to bridge the gap between low-rank adaptation (LoRA) and full fine-tuning, demonstrating its effectiveness in extending context lengths for improved performance, particularly in question-answering tasks.\n",
      "\n",
      "On the other hand, the LoftQ paper introduces an Action Units Relation Learning framework that includes the Action Units Relation Transformer (ART) and the Tampered AU Prediction (TAP). It focuses on modeling relations between facial action units, generating challenging pseudo-samples for model learning, achieving top performance on evaluations, and offering visualizations of tampered regions. The key contributions are the ART encoder for intra-face relations and the TAP process for creating difficult pseudo-samples to enhance model generalization.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Compare and contrast the LoRA papers (LongLoRA, LoftQ). \"\n",
    "    \"Analyze the approach in each paper first. \"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
